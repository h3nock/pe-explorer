# Model and Training Configurations
#
# Design principles:
#   - d_head = 64 for all sizes (2024 best practice for <1B models)
#   - d_ff = 4 × d_model (standard transformer ratio)
#   - Chinchilla-optimal: 20 tokens per parameter (Budget A), 40N (Budget B)
#   - Based on Pythia, SmolLM, MobileLLM papers

tiny:  # ~59M params
  d_model: 512
  n_layers: 6
  n_heads: 8         # 512 / 64 = 8
  d_ff: 2048         # 4 × 512
  max_seq_len: 1024
  max_token_budget: 2_400_000_000
  batch_size: 64
  learning_rate: 6.0e-4
  target_budgets: [1_200_000_000, 2_400_000_000]  # 20N, 40N

small:  # ~164M params
  d_model: 768
  n_layers: 12
  n_heads: 12        # 768 / 64 = 12
  d_ff: 3072         # 4 × 768
  max_seq_len: 1024
  max_token_budget: 6_600_000_000
  batch_size: 32
  learning_rate: 3.0e-4
  target_budgets: [3_300_000_000, 6_600_000_000]  # 20N, 40N

medium:  # ~470M params
  d_model: 1024
  n_layers: 24
  n_heads: 16        # 1024 / 64 = 16
  d_ff: 4096         # 4 × 1024
  max_seq_len: 1024
  max_token_budget: 18_800_000_000
  batch_size: 16
  learning_rate: 2.0e-4
  target_budgets: [9_400_000_000, 18_800_000_000]  # 20N, 40N

large:  # ~1.21B params
  d_model: 2048
  n_layers: 16
  n_heads: 32        # 2048 / 64 = 32
  d_ff: 8192         # 4 × 2048
  max_seq_len: 1024
  max_token_budget: 48_400_000_000
  batch_size: 8
  learning_rate: 1.0e-4
  target_budgets: [24_200_000_000, 48_400_000_000]  # 20N, 40N


# shared training settings
training:
  optimizer: adamw
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  warmup_steps: 2000
  wsd_stage: full     # Options: 'full' (warmup + stable + 10% auto-decay), 'decay_only' (resume & 10% decay)
  decay_tokens: null  # auto-calculated (10%) if null
  grad_clip: 1.0
  dtype: bfloat16
  vocab_size: 65536
  dropout: 0.0
  tie_embedding: true  # tie input/output embeddings (reduces params)
  
  pe_params: {}        # PE-specific params (e.g., {theta: 10000} for RoPE)

# online validation (during training)
validation:
  interval: 500           # validate every N steps
  token_budget: 1_000_000 # tokens to validate on
  log_interval: 10        # log training metrics every N steps
  checkpoint_interval: 5000

# offline evaluation (post-training)
evaluation:
  ppl:
    datasets: [wikitext103, pg19]
    context_lengths: [512, 1024, 2048, 4096, 8192]
    max_tokens: 1_000_000
  algorithmic:
    tasks: [passkey, copy_distance, reverse, sort, no_carry_add, simple_copy]
    max_examples_per_split: 500
    modes: [zero_shot, few_shot]
  passkey:
    context_lengths: [512, 1024, 2048, 4096, 8192]
    samples_per_context: 100
    positions: [beginning, middle, end]
  niah:
    context_lengths: [512, 1024, 2048, 4096, 8192]
    position_percentages: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
    samples_per_cell: 20

data:
  # FineWeb-Edu source 
  # Partitioning: training and filler shards are DISJOINT to avoid duplicate training
  fineweb:
    repo: "karpathy/fineweb-edu-100b-shuffle"
    total_shards: 1822           # total available (100B tokens)
    download_shards: 911         # our default download (50B tokens, ~89GB)
    tokens_per_shard: 55_000_000 # ~55M tokens per shard
    tokenized_path: "data/fineweb_bin"
    # Shard partitioning (indices are 0-based, inclusive, DISJOINT)
    training_shards: [0, 880]      # 881 shards for LM training (~48.4B tokens)
    validation_shards: [881, 890]  # 10 shards for validation (~550M tokens)
    filler_shards: [891, 910]      # 20 shards for synthetic filler (~1.1B tokens)
  
  # mixed training 
  mix_ratio:
    # ratios are fractions (e.g., 0.9 not 90); interleaving is deterministic and
    # caps total samples to the smallest dataset relative to its weight. ensure each dataset has enough samples for its share of the run.
    fineweb: 0.9
    algorithmic: 0.1
  algorithmic:
    train_path: "data/algorithmic/train.parquet"
    eval_path: "data/algorithmic/eval.parquet"
    tokenized_path: "data/algorithmic_bin/train"
    col_name: "text"
