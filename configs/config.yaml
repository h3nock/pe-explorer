# Model and Training Configurations
#
# Design principles:
#   - d_head = 64 for all sizes (2024 best practice for <1B models)
#   - d_ff = 4 × d_model (standard transformer ratio)
#   - n_layers derived from: params ≈ 12 × n_layers × d_model²
#   - Chinchilla-optimal: 20 tokens per parameter (Budget A), 40N (Budget B)
#   - Based on Pythia, SmolLM, MobileLLM papers

tiny:
  d_model: 512
  n_layers: 6
  n_heads: 8         # 512 / 64 = 8
  d_ff: 2048         # 4 × 512
  max_seq_len: 1024
  max_token_budget: 2_800_000_000
  batch_size: 64
  learning_rate: 6.0e-4
  target_budgets: [1_400_000_000, 2_800_000_000]  # 20N (~70M), 40N

small:
  d_model: 768
  n_layers: 12
  n_heads: 12        # 768 / 64 = 12
  d_ff: 3072         # 4 × 768
  max_seq_len: 1024
  max_token_budget: 6_400_000_000
  batch_size: 32
  learning_rate: 3.0e-4
  target_budgets: [3_200_000_000, 6_400_000_000]  # 20N (~160M), 40N

medium:
  d_model: 1024
  n_layers: 24
  n_heads: 16        # 1024 / 64 = 16
  d_ff: 4096         # 4 × 1024
  max_seq_len: 1024
  max_token_budget: 16_400_000_000
  batch_size: 16
  learning_rate: 2.0e-4
  target_budgets: [8_200_000_000, 16_400_000_000]  # 20N (~410M), 40N

large:
  d_model: 2048
  n_layers: 16
  n_heads: 32        # 2048 / 64 = 32
  d_ff: 8192         # 4 × 2048
  max_seq_len: 1024
  max_token_budget: 40_000_000_000
  batch_size: 8
  learning_rate: 1.0e-4
  target_budgets: [20_000_000_000, 40_000_000_000]  # 20N (~1B), 40N


# shared training settings
training:
  optimizer: adamw
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  warmup_steps: 2000 
  wsd_stage: full     # Options: 'full' (warmup + stable + 10% auto-decay), 'decay_only' (resume & 10% decay)
  decay_tokens: null  # auto-calculated (10%) if null
  grad_clip: 1.0
  dtype: bfloat16
  vocab_size: 50257 
  dropout: 0.0 

evaluation:
  # training-time eval 
  eval_interval: 500
  eval_tokens: 1_000_000
  log_interval: 10
  checkpoint_interval: 5000
  
  # post-training eval settings
  post_training:
    # tier 1: perplexity
    ppl:
      datasets: [wikitext103, pg19]
      context_lengths: [512, 1024, 2048, 4096, 8192]  # includes extrapolation
      max_tokens: 1_000_000
    
    # tier 2: algorithmic
    algorithmic:
      tasks: [passkey, copy_distance, reverse, sort, no_carry_add, simple_copy]
      max_examples_per_split: 500
      modes: [zero_shot, few_shot]
    
    # tier 3: position-dependent retrieval
    passkey:
      context_lengths: [512, 1024, 2048, 4096, 8192]
      samples_per_context: 100
      positions: [beginning, middle, end]
    niah:  # needle-in-a-haystack sweep
      context_lengths: [512, 1024, 2048, 4096, 8192]
      position_percentages: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
      samples_per_cell: 20

data:
  # FineWeb-Edu source 
  # Partitioning: training and filler shards are DISJOINT to avoid duplicate training
  fineweb:
    repo: "karpathy/fineweb-edu-100b-shuffle"
    total_shards: 1822           # total available (100B tokens)
    download_shards: 911         # our default download (50B tokens, ~89GB)
    tokens_per_shard: 55_000_000 # ~55M tokens per shard
    tokenized_path: "data/fineweb_bin"
    # Shard partitioning (indices are 0-based, inclusive, DISJOINT)
    training_shards: [0, 880]      # 881 shards for LM training (~48.4B tokens)
    validation_shards: [881, 890]  # 10 shards for validation (~550M tokens)
    filler_shards: [891, 910]      # 20 shards for synthetic filler (~1.1B tokens)
  
  # mixed training 
  mix_ratio:
    fineweb: 0.9
    algorithmic: 0.1
  algorithmic:
    train_path: "data/algorithmic/train.parquet"
    eval_path: "data/algorithmic/eval.parquet"
    tokenized_path: "data/algorithmic_bin/train"
    col_name: "text"
